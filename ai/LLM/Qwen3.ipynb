{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34d7044",
   "metadata": {},
   "source": [
    "# Qwen3 0.6B 모델과 감성분석 모델을 합치기 위한 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c58a8",
   "metadata": {},
   "source": [
    "### 1. 모델 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e33426",
   "metadata": {},
   "source": [
    "- huggingface의 transformers를 사용하여 가져올 것\n",
    "- 1차로 제대로 가져와서 모델이 작동하는지 확인 후 LLM 폴더에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606865c8112946f5ba46e4ea52cf819d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\WANG\\Kcvenv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SSAFY\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-0.6B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17da4fd44aa94bf389e4bdb98392ce09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba73bc4ffd408ba23470f04233030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31df6fdc20541608cf131c1020e203b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929dec5eebdb452ea613343747ed1a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72827f893cbf4fdb958238775f5b4d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e38fdb051d49d4ab57a8e7c9af1699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked, \"Who are you?\" I need to respond in a friendly and helpful way. Let me start by acknowledging their question. I should mention that I'm an AI\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"오늘 사용자의 일기에서 관측되는 감정은 합계를 1이라고 했을 때 우울 0.7, 상처 0.17, 불안 0.1, 기쁨0.03 입니다. 이 사용자에게 심리 상담을 해주고 싶은데 어떤 방식으로 할 지 추천해주세요\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50db033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"./qwen3 0.6B\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814f9a1",
   "metadata": {},
   "source": [
    "## 모델을 로컬로 가져온 후 사용 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8675edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_path = './Qwen'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(\"cpu\")\n",
    "\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dae6b4",
   "metadata": {},
   "source": [
    "### 사용자 질문 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc0c2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변 소요 시간 :  52.96225595474243\n",
      "<think>\n",
      "Okay, let's see. The user provided some emotion percentages: 1 for happiness, 0.7 for sadness, 0.17 for anger, 0.1 for fear, and 0.03 for joy. Wait, the total should add up to 1. Let me check: 0.7 + 0.17 + 0.1 + 0.03 + 0.0? No, that adds up to 0.7 + 0.17 = 0.87, plus 0.1 is 0.97, plus 0.03 is 1.00. Oh right, the joy is 0.03. So the total is 1.00. \n",
      "\n",
      "Now, the user wants to give advice for a psychological consultation. First, I need to understand the emotions. Sadness is 0.7, which is quite high. Anger 0.17, fear 0.1, and joy 0.03. The user might be feeling overwhelmed or stressed. \n",
      "\n",
      "I should start by acknowledging the emotions. Then, maybe break them down. For sadness, suggest ways to process it, like journaling or talking. Anger could relate to stress, so perhaps recommend stress management techniques. Fear might be related to anxiety, so addressing that could help. \n",
      "\n",
      "Also, the percentages indicate that the user is experiencing a mix of emotions. It's important to address both the intensity and the nature of each emotion. Maybe suggest a step-by-step approach, like first acknowledging the emotions, then exploring the triggers, and finally seeking professional help if needed. \n",
      "\n",
      "I should make sure the advice is empathetic and practical. Avoid jargon, keep it clear. Maybe also mention the importance of self-compassion and self-compassion in the process. \n",
      "\n",
      "Wait, the user mentioned the total is 1, so maybe\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"오늘 사용자의 일기에서 관측되는 감정은 합계를 1이라고 했을 때 우울 0.7, 상처 0.17, 불안 0.1, 기쁨0.03 입니다. 이 사용자에게 심리 상담을 해주고 싶은데 어떤 방식으로 할 지 추천해주세요. 답변은 한국어로 해주세요\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "end = time.time()\n",
    "output_time = end - start\n",
    "\n",
    "print('답변 소요 시간 : ', output_time)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782982db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kcvenv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
