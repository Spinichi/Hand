{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04781b4",
   "metadata": {},
   "source": [
    "### 그래픽 카드 설정 및 확인\n",
    "- 현재 kernal에서 버전 확인이 꼭 필요한 것들 확인\n",
    "- torch 버전이 2.5.1+cu121이 아닐 경우 아래의 주석 해제 후 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8069fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.14.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: torch\n",
      "Version: 2.5.1+cu121\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, peft, torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: transformers\n",
      "Version: 4.57.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: mlflow\n",
      "Version: 3.5.1\n",
      "Summary: MLflow is an open source platform for the complete machine learning lifecycle\n",
      "Home-page: https://mlflow.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: Copyright 2018 Databricks, Inc.  All rights reserved.\n",
      "        \n",
      "                                        Apache License\n",
      "                                   Version 2.0, January 2004\n",
      "                                http://www.apache.org/licenses/\n",
      "        \n",
      "           TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
      "        \n",
      "           1. Definitions.\n",
      "        \n",
      "              \"License\" shall mean the terms and conditions for use, reproduction,\n",
      "              and distribution as defined by Sections 1 through 9 of this document.\n",
      "        \n",
      "              \"Licensor\" shall mean the copyright owner or entity authorized by\n",
      "              the copyright owner that is granting the License.\n",
      "        \n",
      "              \"Legal Entity\" shall mean the union of the acting entity and all\n",
      "              other entities that control, are controlled by, or are under common\n",
      "              control with that entity. For the purposes of this definition,\n",
      "              \"control\" means (i) the power, direct or indirect, to cause the\n",
      "              direction or management of such entity, whether by contract or\n",
      "              otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
      "              outstanding shares, or (iii) beneficial ownership of such entity.\n",
      "        \n",
      "              \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
      "              exercising permissions granted by this License.\n",
      "        \n",
      "              \"Source\" form shall mean the preferred form for making modifications,\n",
      "              including but not limited to software source code, documentation\n",
      "              source, and configuration files.\n",
      "        \n",
      "              \"Object\" form shall mean any form resulting from mechanical\n",
      "              transformation or translation of a Source form, including but\n",
      "              not limited to compiled object code, generated documentation,\n",
      "              and conversions to other media types.\n",
      "        \n",
      "              \"Work\" shall mean the work of authorship, whether in Source or\n",
      "              Object form, made available under the License, as indicated by a\n",
      "              copyright notice that is included in or attached to the work\n",
      "              (an example is provided in the Appendix below).\n",
      "        \n",
      "              \"Derivative Works\" shall mean any work, whether in Source or Object\n",
      "              form, that is based on (or derived from) the Work and for which the\n",
      "              editorial revisions, annotations, elaborations, or other modifications\n",
      "              represent, as a whole, an original work of authorship. For the purposes\n",
      "              of this License, Derivative Works shall not include works that remain\n",
      "              separable from, or merely link (or bind by name) to the interfaces of,\n",
      "              the Work and Derivative Works thereof.\n",
      "        \n",
      "              \"Contribution\" shall mean any work of authorship, including\n",
      "              the original version of the Work and any modifications or additions\n",
      "              to that Work or Derivative Works thereof, that is intentionally\n",
      "              submitted to Licensor for inclusion in the Work by the copyright owner\n",
      "              or by an individual or Legal Entity authorized to submit on behalf of\n",
      "              the copyright owner. For the purposes of this definition, \"submitted\"\n",
      "              means any form of electronic, verbal, or written communication sent\n",
      "              to the Licensor or its representatives, including but not limited to\n",
      "              communication on electronic mailing lists, source code control systems,\n",
      "              and issue tracking systems that are managed by, or on behalf of, the\n",
      "              Licensor for the purpose of discussing and improving the Work, but\n",
      "              excluding communication that is conspicuously marked or otherwise\n",
      "              designated in writing by the copyright owner as \"Not a Contribution.\"\n",
      "        \n",
      "              \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
      "              on behalf of whom a Contribution has been received by Licensor and\n",
      "              subsequently incorporated within the Work.\n",
      "        \n",
      "           2. Grant of Copyright License. Subject to the terms and conditions of\n",
      "              this License, each Contributor hereby grants to You a perpetual,\n",
      "              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "              copyright license to reproduce, prepare Derivative Works of,\n",
      "              publicly display, publicly perform, sublicense, and distribute the\n",
      "              Work and such Derivative Works in Source or Object form.\n",
      "        \n",
      "           3. Grant of Patent License. Subject to the terms and conditions of\n",
      "              this License, each Contributor hereby grants to You a perpetual,\n",
      "              worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
      "              (except as stated in this section) patent license to make, have made,\n",
      "              use, offer to sell, sell, import, and otherwise transfer the Work,\n",
      "              where such license applies only to those patent claims licensable\n",
      "              by such Contributor that are necessarily infringed by their\n",
      "              Contribution(s) alone or by combination of their Contribution(s)\n",
      "              with the Work to which such Contribution(s) was submitted. If You\n",
      "              institute patent litigation against any entity (including a\n",
      "              cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
      "              or a Contribution incorporated within the Work constitutes direct\n",
      "              or contributory patent infringement, then any patent licenses\n",
      "              granted to You under this License for that Work shall terminate\n",
      "              as of the date such litigation is filed.\n",
      "        \n",
      "           4. Redistribution. You may reproduce and distribute copies of the\n",
      "              Work or Derivative Works thereof in any medium, with or without\n",
      "              modifications, and in Source or Object form, provided that You\n",
      "              meet the following conditions:\n",
      "        \n",
      "              (a) You must give any other recipients of the Work or\n",
      "                  Derivative Works a copy of this License; and\n",
      "        \n",
      "              (b) You must cause any modified files to carry prominent notices\n",
      "                  stating that You changed the files; and\n",
      "        \n",
      "              (c) You must retain, in the Source form of any Derivative Works\n",
      "                  that You distribute, all copyright, patent, trademark, and\n",
      "                  attribution notices from the Source form of the Work,\n",
      "                  excluding those notices that do not pertain to any part of\n",
      "                  the Derivative Works; and\n",
      "        \n",
      "              (d) If the Work includes a \"NOTICE\" text file as part of its\n",
      "                  distribution, then any Derivative Works that You distribute must\n",
      "                  include a readable copy of the attribution notices contained\n",
      "                  within such NOTICE file, excluding those notices that do not\n",
      "                  pertain to any part of the Derivative Works, in at least one\n",
      "                  of the following places: within a NOTICE text file distributed\n",
      "                  as part of the Derivative Works; within the Source form or\n",
      "                  documentation, if provided along with the Derivative Works; or,\n",
      "                  within a display generated by the Derivative Works, if and\n",
      "                  wherever such third-party notices normally appear. The contents\n",
      "                  of the NOTICE file are for informational purposes only and\n",
      "                  do not modify the License. You may add Your own attribution\n",
      "                  notices within Derivative Works that You distribute, alongside\n",
      "                  or as an addendum to the NOTICE text from the Work, provided\n",
      "                  that such additional attribution notices cannot be construed\n",
      "                  as modifying the License.\n",
      "        \n",
      "              You may add Your own copyright statement to Your modifications and\n",
      "              may provide additional or different license terms and conditions\n",
      "              for use, reproduction, or distribution of Your modifications, or\n",
      "              for any such Derivative Works as a whole, provided Your use,\n",
      "              reproduction, and distribution of the Work otherwise complies with\n",
      "              the conditions stated in this License.\n",
      "        \n",
      "           5. Submission of Contributions. Unless You explicitly state otherwise,\n",
      "              any Contribution intentionally submitted for inclusion in the Work\n",
      "              by You to the Licensor shall be under the terms and conditions of\n",
      "              this License, without any additional terms or conditions.\n",
      "              Notwithstanding the above, nothing herein shall supersede or modify\n",
      "              the terms of any separate license agreement you may have executed\n",
      "              with Licensor regarding such Contributions.\n",
      "        \n",
      "           6. Trademarks. This License does not grant permission to use the trade\n",
      "              names, trademarks, service marks, or product names of the Licensor,\n",
      "              except as required for reasonable and customary use in describing the\n",
      "              origin of the Work and reproducing the content of the NOTICE file.\n",
      "        \n",
      "           7. Disclaimer of Warranty. Unless required by applicable law or\n",
      "              agreed to in writing, Licensor provides the Work (and each\n",
      "              Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
      "              WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
      "              implied, including, without limitation, any warranties or conditions\n",
      "              of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
      "              PARTICULAR PURPOSE. You are solely responsible for determining the\n",
      "              appropriateness of using or redistributing the Work and assume any\n",
      "              risks associated with Your exercise of permissions under this License.\n",
      "        \n",
      "           8. Limitation of Liability. In no event and under no legal theory,\n",
      "              whether in tort (including negligence), contract, or otherwise,\n",
      "              unless required by applicable law (such as deliberate and grossly\n",
      "              negligent acts) or agreed to in writing, shall any Contributor be\n",
      "              liable to You for damages, including any direct, indirect, special,\n",
      "              incidental, or consequential damages of any character arising as a\n",
      "              result of this License or out of the use or inability to use the\n",
      "              Work (including but not limited to damages for loss of goodwill,\n",
      "              work stoppage, computer failure or malfunction, or any and all\n",
      "              other commercial damages or losses), even if such Contributor\n",
      "              has been advised of the possibility of such damages.\n",
      "        \n",
      "           9. Accepting Warranty or Additional Liability. While redistributing\n",
      "              the Work or Derivative Works thereof, You may choose to offer,\n",
      "              and charge a fee for, acceptance of support, warranty, indemnity,\n",
      "              or other liability obligations and/or rights consistent with this\n",
      "              License. However, in accepting such obligations, You may act only\n",
      "              on Your own behalf and on Your sole responsibility, not on behalf\n",
      "              of any other Contributor, and only if You agree to indemnify,\n",
      "              defend, and hold each Contributor harmless for any liability\n",
      "              incurred by, or claims asserted against, such Contributor by reason\n",
      "              of your accepting any such warranty or additional liability.\n",
      "        \n",
      "           END OF TERMS AND CONDITIONS\n",
      "           APPENDIX: How to apply the Apache License to your work.\n",
      "        \n",
      "              To apply the Apache License to your work, attach the following\n",
      "              boilerplate notice, with the fields enclosed by brackets \"[]\"\n",
      "              replaced with your own identifying information. (Don't include\n",
      "              the brackets!)  The text should be enclosed in the appropriate\n",
      "              comment syntax for the file format. We also recommend that a\n",
      "              file or class name and description of purpose be included on the\n",
      "              same \"printed page\" as the copyright notice for easier\n",
      "              identification within third-party archives.\n",
      "        \n",
      "           Copyright [yyyy] [name of copyright owner]\n",
      "        \n",
      "           Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "           you may not use this file except in compliance with the License.\n",
      "           You may obtain a copy of the License at\n",
      "        \n",
      "               http://www.apache.org/licenses/LICENSE-2.0\n",
      "        \n",
      "           Unless required by applicable law or agreed to in writing, software\n",
      "           distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "           WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "           See the License for the specific language governing permissions and\n",
      "           limitations under the License.\n",
      "        \n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: alembic, cryptography, docker, Flask, Flask-CORS, graphene, matplotlib, mlflow-skinny, mlflow-tracing, numpy, pandas, pyarrow, scikit-learn, scipy, sqlalchemy, waitress\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show tensorflow\n",
    "%pip show torch\n",
    "%pip show transformers\n",
    "%pip show mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aa07d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)       # 2.5.1\n",
    "print(torch.version.cuda)      # 12.1\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fabb2",
   "metadata": {},
   "source": [
    "토치 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b7d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743a38c",
   "metadata": {},
   "source": [
    "CUDA 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525b52ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b6ac6",
   "metadata": {},
   "source": [
    "### 로컬에선 GPU 지정 필요없음. GPU 서버에서는 주석 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9725c23-237d-4286-8a98-98dbb371a301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df27cf1",
   "metadata": {},
   "source": [
    "- 현재 파일에서는 hugginface의 beomi/KcELECTRA-base 모델을 사용할 예정입니다.\n",
    "- 해당 모델에 관한 정보는 config 파일을 통해 관리합니다.\n",
    "- 다른 모델을 사용하기 원할 경우 config의 MODEL_NAME을 huggingface의 가이드에 따라 변경하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ccfec",
   "metadata": {},
   "source": [
    "## 성능 향상을 위한 함수\n",
    "1. Text 정제\n",
    "- huggingface에서 개발자가 공개한 성능을 높이기 위한 데이터 전처리 과정입니다. 특수문자 및 이모지 등을 제거합니다\n",
    "- 불용어는 제거하지 않습니다. 감정 모델에서는 불용어 또한 중요한 데이터일 수 있습니다.\n",
    "2. KcELECTRA 최적화인 64 토큰으로 처리하기 위한 함수 설정\n",
    "- 논문 결과에 KcELECTRA는 토큰의 길이가 64일 경우 성능이 가장 최대로 나옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbce7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.EMOJI_DATA.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x): \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = emoji.replace_emoji(x, replace='') #emoji 삭제\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def chunk_by_tokens(text, tokenizer, max_len=64, stride=16):\n",
    "    \"\"\"문자열을 토큰 길이 기준으로 슬라이딩 윈도우 분할\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_len - 2, len(tokens))\n",
    "        input_ids = (\n",
    "            [tokenizer.cls_token_id]\n",
    "            + tokens[start:end]\n",
    "            + [tokenizer.sep_token_id]\n",
    "        )\n",
    "        chunk_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - stride  # stride 만큼 겹치게 이동\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f80ef",
   "metadata": {},
   "source": [
    "### 텍스트 정제 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e455bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 오늘 선배한테 혼났어 .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"나는 오늘 선배한테 혼났어 ★.\"\n",
    "cleaned_sentence = clean(sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e62274",
   "metadata": {},
   "source": [
    "#### 감정 매핑."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb656dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_label = {\n",
    "    \"happy\" : \"기쁨\",\n",
    "    \"anger\" : \"분노\",\n",
    "    \"unrest\" : \"불안\",\n",
    "    \"embarrass\" : \"당황\",\n",
    "    \"sadness\" : \"슬픔\",\n",
    "    \"damaged\" : \"상처\"\n",
    "}\n",
    "\n",
    "Emotion_Classification = list(sentiment_label.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8699216",
   "metadata": {},
   "source": [
    "## 학습 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd79d0f",
   "metadata": {},
   "source": [
    "#### 1. 모델 설정을 위한 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a440bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: protobuf\n",
      "Version: 4.25.8\n",
      "Summary: \n",
      "Home-page: https://developers.google.com/protocol-buffers/\n",
      "Author: protobuf@googlegroups.com\n",
      "Author-email: protobuf@googlegroups.com\n",
      "License: 3-Clause BSD License\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: databricks-sdk, mlflow-skinny, mlflow-tracing, opentelemetry-proto, tensorboard, tensorflow-intel\n"
     ]
    }
   ],
   "source": [
    "!pip show protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1ae5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.utils.move_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2104beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.45.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft\n",
      "---\n",
      "Name: accelerate\n",
      "Version: 0.34.2\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: peft\n",
      "---\n",
      "Name: peft\n",
      "Version: 0.17.1\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\n",
      "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show transformers accelerate peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3a568c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'is_grokadamw_available' from 'transformers.utils' (g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mg:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1586\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mg:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\trainer.py:136\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptimizerNames, ParallelMode, TrainingArguments\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    137\u001b[0m     ADAPTER_CONFIG_NAME,\n\u001b[0;32m    138\u001b[0m     ADAPTER_SAFE_WEIGHTS_NAME,\n\u001b[0;32m    139\u001b[0m     ADAPTER_WEIGHTS_NAME,\n\u001b[0;32m    140\u001b[0m     CONFIG_NAME,\n\u001b[0;32m    141\u001b[0m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[0;32m    142\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[0;32m    143\u001b[0m     WEIGHTS_INDEX_NAME,\n\u001b[0;32m    144\u001b[0m     WEIGHTS_NAME,\n\u001b[0;32m    145\u001b[0m     XLA_FSDPV2_MIN_VERSION,\n\u001b[0;32m    146\u001b[0m     PushInProgress,\n\u001b[0;32m    147\u001b[0m     PushToHubMixin,\n\u001b[0;32m    148\u001b[0m     can_return_loss,\n\u001b[0;32m    149\u001b[0m     find_labels,\n\u001b[0;32m    150\u001b[0m     is_accelerate_available,\n\u001b[0;32m    151\u001b[0m     is_apex_available,\n\u001b[0;32m    152\u001b[0m     is_bitsandbytes_available,\n\u001b[0;32m    153\u001b[0m     is_datasets_available,\n\u001b[0;32m    154\u001b[0m     is_galore_torch_available,\n\u001b[0;32m    155\u001b[0m     is_grokadamw_available,\n\u001b[0;32m    156\u001b[0m     is_in_notebook,\n\u001b[0;32m    157\u001b[0m     is_ipex_available,\n\u001b[0;32m    158\u001b[0m     is_liger_kernel_available,\n\u001b[0;32m    159\u001b[0m     is_lomo_available,\n\u001b[0;32m    160\u001b[0m     is_peft_available,\n\u001b[0;32m    161\u001b[0m     is_safetensors_available,\n\u001b[0;32m    162\u001b[0m     is_sagemaker_dp_enabled,\n\u001b[0;32m    163\u001b[0m     is_sagemaker_mp_enabled,\n\u001b[0;32m    164\u001b[0m     is_schedulefree_available,\n\u001b[0;32m    165\u001b[0m     is_torch_compile_available,\n\u001b[0;32m    166\u001b[0m     is_torch_mlu_available,\n\u001b[0;32m    167\u001b[0m     is_torch_mps_available,\n\u001b[0;32m    168\u001b[0m     is_torch_musa_available,\n\u001b[0;32m    169\u001b[0m     is_torch_neuroncore_available,\n\u001b[0;32m    170\u001b[0m     is_torch_npu_available,\n\u001b[0;32m    171\u001b[0m     is_torch_xla_available,\n\u001b[0;32m    172\u001b[0m     is_torch_xpu_available,\n\u001b[0;32m    173\u001b[0m     is_torchao_available,\n\u001b[0;32m    174\u001b[0m     logging,\n\u001b[0;32m    175\u001b[0m     strtobool,\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QuantizationMethod\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_grokadamw_available' from 'transformers.utils' (g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\utils\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support, f1_score\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     AutoTokenizer,\n\u001b[0;32m     10\u001b[0m     AutoModelForSequenceClassification,\n\u001b[0;32m     11\u001b[0m     TrainingArguments,\n\u001b[0;32m     12\u001b[0m     Trainer,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mconfig_train\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelConfig \u001b[38;5;28;01mas\u001b[39;00m cf\n\u001b[0;32m     16\u001b[0m model_name \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mMODEL_NAME\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mg:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1576\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1574\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1576\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1577\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mg:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1588\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1589\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1591\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'is_grokadamw_available' from 'transformers.utils' (g:\\싸피\\코드 관련\\.venv\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from config_train import ModelConfig as cf\n",
    "\n",
    "model_name = cf.MODEL_NAME\n",
    "# 전처리한 json 파일 절대 경로(상대도 가능할듯?)\n",
    "DATA_DIR = r\"G:\\싸피\\코드 관련\\S13P31A106\\ai\\Data\\018.감성대화\"\n",
    "OUTPUT_DIR = r\"G:\\싸피\\코드 관련\\S13P31A106\\ai\\Data\\018.감성대화\\model_checkpoint\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 모델 설정. 나중에도 \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(Emotion_Classification),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4e64c",
   "metadata": {},
   "source": [
    "#### 1-2. LoRA를 붙이기 위해 모델명 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ebe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lora_target_modules(model, patterns=None, min_hits=10):\n",
    "    \"\"\"\n",
    "    Electra 계열의 self-attention 쿼리/키/밸류/출력 dense 레이어를 자동 탐색.\n",
    "    patterns: 포함시킬 하위 모듈명 키워드 리스트 (정규식 포함 가능)\n",
    "    min_hits: 너무 적게 잡히면 경고용\n",
    "    \"\"\"\n",
    "    if patterns is None:\n",
    "        # Electra(HF)에서 흔히 보이는 명칭들\n",
    "        patterns = [r\"\\.query$\", r\"\\.key$\", r\"\\.value$\", r\"\\.dense$\"]\n",
    "\n",
    "    names = []\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            for p in patterns:\n",
    "                if re.search(p, n):\n",
    "                    names.append(n.split(\".\")[-1])  # 마지막 토큰만 추출 (PEFT는 leaf 모듈명 지정 선호)\n",
    "                    break\n",
    "\n",
    "    # leaf 모듈명만 추출했기 때문에 중복 제거\n",
    "    names = sorted(list(set(names)))\n",
    "\n",
    "    # Electra 구현에 따라 self.dense / output.dense 등 혼재 -> 최소세트 보정\n",
    "    if len(names) < 3:\n",
    "        print(\"[WARN] LoRA target modules detected too few:\", names)\n",
    "\n",
    "    print(\"[LoRA target modules]\", names)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93577e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 설정되어있는 경우에만 사용, 현재는 이렇게 되어있음.\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=8,\n",
    "#     problem_type=\"multi_label_classification\"\n",
    "# )\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "import mlflow\n",
    "import os, re, torch\n",
    "\n",
    "# Electra 계열 타깃 모듈 자동 탐색\n",
    "target_modules = find_lora_target_modules(model)  # 대체로 ['query','key','value','dense'] 식으로 나옴\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,   # 시퀀스 분류\n",
    "    r=8,                          # 랭크 (메모리/속도/성능 절충)\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules # 위에서 탐색한 모듈에만 로라 적용\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # trainable params 점검\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dbc27",
   "metadata": {},
   "source": [
    "#### 1-3. MLflow 추적을 위한 기본 세팅\n",
    "- 학습 함수의 training_args에 mlflow로 삽입은 해뒀음.\n",
    "- GPU 서버에서 돌릴 경우 서버와 로컬을 연결하는 파이프라인 설정 필요.\n",
    "- GPU 서버쪽에서 따로 열어보는건 추후에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:5000\"\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"KcELECTRA_emotion_finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317163a9",
   "metadata": {},
   "source": [
    "#### 2. 추론 카테고리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 카테고리\n",
    "CATEGORIES = [\n",
    "    \"happy\",\n",
    "    \"anger\",\n",
    "    \"unrest\",\n",
    "    \"embarrass\",\n",
    "    \"sadness\",\n",
    "    \"damaged\"\n",
    "]\n",
    "NUM_LABELS = len(CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb5873",
   "metadata": {},
   "source": [
    "#### 3. 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    # 논문상에선 시퀀스가 64에서 최적의 성능을 뽑아낸다고 했으나 단순 감정 학습이 아니라 상담 데이터 학습이므로 일단 128로 설정. 추후 실험 예정.\n",
    "    def __init__(self, json_files, tokenizer, max_len=cf.SEQUENCE_LEN, stride=cf.SEQUENCE_STRIDE_LEN):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.stride = stride\n",
    "\n",
    "        for path in json_files:\n",
    "            with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "                samples = json.load(f)\n",
    "\n",
    "            for s in samples:\n",
    "                if not s.get(\"text\") or not s.get(\"labels\"):\n",
    "                    continue\n",
    "                \n",
    "                if type(s[\"text\"]) == int:\n",
    "                    continue\n",
    "                else:\n",
    "                    raw_text = clean(s[\"text\"])\n",
    "                if ('네' in raw_text or '아니오' in raw_text) and len(raw_text) <= 5:\n",
    "                    continue\n",
    "                labels = s[\"labels\"]\n",
    "                y = [labels.get(cat, 0) for cat in CATEGORIES]\n",
    "\n",
    "                # 토큰 기준 분할\n",
    "                for chunk_text in chunk_by_tokens(raw_text, tokenizer, max_len, stride):\n",
    "                    self.data.append({\"text\": chunk_text, \"labels\": y})\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.data)} total samples (after chunking)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d5673",
   "metadata": {},
   "source": [
    "#### 4.학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18faacbb",
   "metadata": {},
   "source": [
    "training_args 부분만 LoRA 적용을 위해 수정.\n",
    "학습 데이터는 MLflow에서 받아볼 거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, __version__ as tfv\n",
    "import inspect\n",
    "print(\"transformers version:\", tfv)\n",
    "print(\"TrainingArguments module:\", TrainingArguments.__module__)\n",
    "print(\"TrainingArguments init signature:\", inspect.signature(TrainingArguments.__init__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    # Multi-label classification용 sigmoid 적용\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs > 0.5).astype(int)   # 0.5 기준으로 0/1 예측\n",
    "\n",
    "    # 각종 metric 계산\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 파일 분할 (세션 단위)\n",
    "files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
    "random.shuffle(files)\n",
    "split_idx = int(len(files) * 0.8)\n",
    "train_files, val_files = files[:split_idx], files[split_idx:]\n",
    "\n",
    "train_ds = EmotionDataset(train_files, tokenizer)\n",
    "val_ds = EmotionDataset(val_files, tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,              # 기존 경로\n",
    "    per_device_train_batch_size=cf.TRAIN_BATCH_SIZE,     \n",
    "    per_device_eval_batch_size=cf.EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=cf.LEARNING_RATE,                 # LoRA면 3e-5 ~ 5e-5 권장 (데이터 따라 튜닝)\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",         # compute_metrics에서 f1 반환한다고 가정\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"mlflow\"],               # MLflow에서 report 받아서 확인할거임.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"best_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"best_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8b3b8",
   "metadata": {},
   "source": [
    "#### 5. 어댑터 설정 - 얘는 ONNX 등의 변환이 필요하면 쓰고 아니면 말거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (1) 어댑터만 저장\n",
    "# adapter_dir = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
    "# trainer.model.save_pretrained(adapter_dir)\n",
    "# tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "# # 추론 시: 기반 모델 + 어댑터 로드\n",
    "# # base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8, problem_type=\"multi_label_classification\")\n",
    "# # model_lora = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "\n",
    "# # (2) 병합해서 일반 모델로 내보내기 (필요할 때)\n",
    "# merged_dir = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "# model_lora = trainer.model\n",
    "# merged = model_lora.merge_and_unload()   # LoRA 가중치를 베이스에 병합\n",
    "# merged.save_pretrained(merged_dir)\n",
    "# tokenizer.save_pretrained(merged_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0503d1",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text: str):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "    return {CATEGORIES[i]: round(float(probs[i]), 3) for i in range(NUM_LABELS)}\n",
    "\n",
    "print(predict_emotion(\"요즘 너무 무기력하고 잠도 잘 안 와요.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
