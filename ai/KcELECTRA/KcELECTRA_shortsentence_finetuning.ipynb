{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04781b4",
   "metadata": {},
   "source": [
    "### 그래픽 카드 설정 및 확인\n",
    "**torch로 그래픽카드가 잡히지 않을 때만 확인용으로 주석 제거 후 돌리기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22b5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install -q peft mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fabb2",
   "metadata": {},
   "source": [
    "토치 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf78f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.5.1+cu121\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: accelerate, bitsandbytes, peft, torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.Name: transformers\n",
      "Version: 4.57.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, trl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ssafy\\desktop\\wang\\kcvenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip show torch\n",
    "%pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.training_args\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743a38c",
   "metadata": {},
   "source": [
    "CUDA 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525b52ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b6ac6",
   "metadata": {},
   "source": [
    "### 로컬에선 GPU 지정 필요없음. GPU 서버에서는 주석 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9725c23-237d-4286-8a98-98dbb371a301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df27cf1",
   "metadata": {},
   "source": [
    "- 현재 파일에서는 hugginface의 beomi/KcELECTRA-base 모델을 사용할 예정입니다.\n",
    "- 해당 모델에 관한 정보는 config 파일을 통해 관리합니다.\n",
    "- 다른 모델을 사용하기 원할 경우 config의 MODEL_NAME을 huggingface의 가이드에 따라 변경하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ccfec",
   "metadata": {},
   "source": [
    "## 성능 향상을 위한 함수\n",
    "1. Text 정제\n",
    "- huggingface에서 개발자가 공개한 성능을 높이기 위한 데이터 전처리 과정입니다. 특수문자 및 이모지 등을 제거합니다\n",
    "- 불용어는 제거하지 않습니다. 감정 모델에서는 불용어 또한 중요한 데이터일 수 있습니다.\n",
    "2. KcELECTRA 최적화인 64 토큰으로 처리하기 위한 함수 설정\n",
    "- 논문 결과에 KcELECTRA는 토큰의 길이가 64일 경우 성능이 가장 최대로 나옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcbce7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "emojis = ''.join(emoji.EMOJI_DATA.keys())\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣]+')\n",
    "url_pattern = re.compile(\n",
    "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
    "\n",
    "def clean(x): \n",
    "    x = pattern.sub(' ', x)\n",
    "    x = emoji.replace_emoji(x, replace='') #emoji 삭제\n",
    "    x = url_pattern.sub('', x)\n",
    "    x = x.strip()\n",
    "    x = repeat_normalize(x, num_repeats=2)\n",
    "    return x\n",
    "\n",
    "def chunk_by_tokens(text, tokenizer, max_len=64, stride=16):\n",
    "    \"\"\"문자열을 토큰 길이 기준으로 슬라이딩 윈도우 분할\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_len - 2, len(tokens))\n",
    "        input_ids = (\n",
    "            [tokenizer.cls_token_id]\n",
    "            + tokens[start:end]\n",
    "            + [tokenizer.sep_token_id]\n",
    "        )\n",
    "        chunk_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - stride  # stride 만큼 겹치게 이동\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f80ef",
   "metadata": {},
   "source": [
    "### 텍스트 정제 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e455bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 오늘 선배한테 혼났어 .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"나는 오늘 선배한테 혼났어 ★.\"\n",
    "cleaned_sentence = clean(sentence)\n",
    "print(cleaned_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e62274",
   "metadata": {},
   "source": [
    "#### 감정 매핑."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb656dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_label = {\n",
    "    \"happy\" : \"기쁨\",\n",
    "    \"anger\" : \"분노\",\n",
    "    \"unrest\" : \"불안\",\n",
    "    \"embarrass\" : \"당황\",\n",
    "    \"sadness\" : \"슬픔\",\n",
    "    \"damaged\" : \"상처\"\n",
    "}\n",
    "\n",
    "Emotion_Classification = list(sentiment_label.keys())\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8699216",
   "metadata": {},
   "source": [
    "## 학습 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd79d0f",
   "metadata": {},
   "source": [
    "#### 1. 모델 설정을 위한 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\SSAFY\\Desktop\\WANG\\Kcvenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from config_train import ModelConfig as cf\n",
    "\n",
    "model_name = cf.MODEL_NAME\n",
    "# 전처리한 json 파일 절대 경로(상대도 가능할듯?)\n",
    "DATA_DIR = r\"C:\\Users\\SSAFY\\Desktop\\WANG\\S13P31A106\\KcELECTRA\\data\\After_processed\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\SSAFY\\Desktop\\WANG\\S13P31A106\\KcELECTRA\\short_checkpoints\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 모델 설정. 나중에도 \n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(Emotion_Classification),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4e64c",
   "metadata": {},
   "source": [
    "#### 1-2. LoRA를 붙이기 위해 모델명 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ebe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lora_target_modules(model, patterns=None, min_hits=10):\n",
    "    \"\"\"\n",
    "    Electra 계열의 self-attention 쿼리/키/밸류/출력 dense 레이어를 자동 탐색.\n",
    "    patterns: 포함시킬 하위 모듈명 키워드 리스트 (정규식 포함 가능)\n",
    "    min_hits: 너무 적게 잡히면 경고용\n",
    "    \"\"\"\n",
    "    if patterns is None:\n",
    "        # Electra(HF)에서 흔히 보이는 명칭들\n",
    "        patterns = [r\"\\.query$\", r\"\\.key$\", r\"\\.value$\", r\"\\.dense$\"]\n",
    "\n",
    "    names = []\n",
    "    for n, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            for p in patterns:\n",
    "                if re.search(p, n):\n",
    "                    names.append(n.split(\".\")[-1])  # 마지막 토큰만 추출 (PEFT는 leaf 모듈명 지정 선호)\n",
    "                    break\n",
    "\n",
    "    # leaf 모듈명만 추출했기 때문에 중복 제거\n",
    "    names = sorted(list(set(names)))\n",
    "\n",
    "    # Electra 구현에 따라 self.dense / output.dense 등 혼재 -> 최소세트 보정\n",
    "    if len(names) < 3:\n",
    "        print(\"[WARN] LoRA target modules detected too few:\", names)\n",
    "\n",
    "    print(\"[LoRA target modules]\", names)\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93577e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LoRA target modules] ['dense', 'key', 'query', 'value']\n",
      "trainable params: 1,936,136 || all params: 111,035,920 || trainable%: 1.7437023982869688\n"
     ]
    }
   ],
   "source": [
    "# 모델이 설정되어있는 경우에만 사용, 현재는 이렇게 되어있음.\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     model_name,\n",
    "#     num_labels=8,\n",
    "#     problem_type=\"multi_label_classification\"\n",
    "# )\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, TaskType\n",
    "import mlflow\n",
    "import os, re, torch\n",
    "\n",
    "# Electra 계열 타깃 모듈 자동 탐색\n",
    "target_modules = find_lora_target_modules(model)  # 대체로 ['query','key','value','dense'] 식으로 나옴\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,   # 시퀀스 분류\n",
    "    r=8,                          # 랭크 (메모리/속도/성능 절충)\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules # 위에서 탐색한 모듈에만 로라 적용\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # trainable params 점검\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dbc27",
   "metadata": {},
   "source": [
    "#### 1-3. MLflow 추적을 위한 기본 세팅\n",
    "- 학습 함수의 training_args에 mlflow로 삽입은 해뒀음.\n",
    "- GPU 서버에서 돌릴 경우 서버와 로컬을 연결하는 파이프라인 설정 필요.\n",
    "- GPU 서버쪽에서 따로 열어보는건 추후에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8c9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://127.0.0.1:5000\"\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"KcELECTRA_emotion_finetuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317163a9",
   "metadata": {},
   "source": [
    "#### 2. 추론 카테고리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감정 카테고리\n",
    "CATEGORIES = [\n",
    "    \"happy\",\n",
    "    \"anger\",\n",
    "    \"unrest\",\n",
    "    \"embarrass\",\n",
    "    \"sadness\",\n",
    "    \"damaged\"\n",
    "]\n",
    "NUM_LABELS = len(CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb5873",
   "metadata": {},
   "source": [
    "#### 3. 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d5e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    # 논문상에선 시퀀스가 64에서 최적의 성능을 뽑아낸다고 했으나 단순 감정 학습이 아니라 상담 데이터 학습이므로 일단 128로 설정. 추후 실험 예정.\n",
    "    def __init__(self, json_files, tokenizer, max_len=cf.SEQUENCE_LEN, stride=cf.SEQUENCE_STRIDE_LEN):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.stride = stride\n",
    "\n",
    "        for path in json_files:\n",
    "            with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "                samples = json.load(f)\n",
    "\n",
    "            for s in samples:\n",
    "                if not s.get(\"text\") or not s.get(\"labels\"):\n",
    "                    continue\n",
    "                \n",
    "                if type(s[\"text\"]) == int:\n",
    "                    continue\n",
    "                else:\n",
    "                    raw_text = clean(s[\"text\"])\n",
    "                if ('네' in raw_text or '아니오' in raw_text) and len(raw_text) <= 5:\n",
    "                    continue\n",
    "                labels = s[\"labels\"]\n",
    "                y = [labels.get(cat, 0) for cat in CATEGORIES]\n",
    "\n",
    "                # 토큰 기준 분할\n",
    "                for chunk_text in chunk_by_tokens(raw_text, tokenizer, max_len, stride):\n",
    "                    self.data.append({\"text\": chunk_text, \"labels\": y})\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.data)} total samples (after chunking)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        enc = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d5673",
   "metadata": {},
   "source": [
    "#### 4.학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18faacbb",
   "metadata": {},
   "source": [
    "training_args 부분만 LoRA 적용을 위해 수정.\n",
    "학습 데이터는 MLflow에서 받아볼 거임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfdf6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.57.1\n",
      "TrainingArguments module: transformers.training_args\n",
      "TrainingArguments init signature: (self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, __version__ as tfv\n",
    "import inspect\n",
    "print(\"transformers version:\", tfv)\n",
    "print(\"TrainingArguments module:\", TrainingArguments.__module__)\n",
    "print(\"TrainingArguments init signature:\", inspect.signature(TrainingArguments.__init__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 18941 total samples (after chunking)\n",
      "✅ Loaded 4932 total samples (after chunking)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_17072\\3618720135.py:50: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='5920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  21/5920 00:08 < 42:46, 2.30 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    # Multi-label classification용 sigmoid 적용\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs > 0.5).astype(int)   # 0.5 기준으로 0/1 예측\n",
    "\n",
    "    # 각종 metric 계산\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"f1_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 파일 분할 (세션 단위)\n",
    "files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(\".json\")]\n",
    "random.shuffle(files)\n",
    "split_idx = int(len(files) * 0.8)\n",
    "train_files, val_files = files[:split_idx], files[split_idx:]\n",
    "\n",
    "train_ds = EmotionDataset(train_files, tokenizer)\n",
    "val_ds = EmotionDataset(val_files, tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,              # 기존 경로\n",
    "    per_device_train_batch_size=cf.TRAIN_BATCH_SIZE,     \n",
    "    per_device_eval_batch_size=cf.EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=cf.LEARNING_RATE,                 # LoRA면 3e-5 ~ 5e-5 권장 (데이터 따라 튜닝)\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",         # compute_metrics에서 f1 반환한다고 가정\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"mlflow\"],               # MLflow에서 report 받아서 확인할거임.\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"best_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"best_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8b3b8",
   "metadata": {},
   "source": [
    "#### 5. 어댑터 설정 - 얘는 ONNX 등의 변환이 필요하면 쓰고 아니면 말거임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (1) 어댑터만 저장\n",
    "# adapter_dir = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
    "# trainer.model.save_pretrained(adapter_dir)\n",
    "# tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "# # 추론 시: 기반 모델 + 어댑터 로드\n",
    "# # base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=8, problem_type=\"multi_label_classification\")\n",
    "# # model_lora = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "\n",
    "# # (2) 병합해서 일반 모델로 내보내기 (필요할 때)\n",
    "# merged_dir = os.path.join(OUTPUT_DIR, \"merged_model\")\n",
    "# model_lora = trainer.model\n",
    "# merged = model_lora.merge_and_unload()   # LoRA 가중치를 베이스에 병합\n",
    "# merged.save_pretrained(merged_dir)\n",
    "# tokenizer.save_pretrained(merged_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0503d1",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text: str):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "    return {CATEGORIES[i]: round(float(probs[i]), 3) for i in range(NUM_LABELS)}\n",
    "\n",
    "print(predict_emotion(\"요즘 너무 무기력하고 잠도 잘 안 와요.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kcvenv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
